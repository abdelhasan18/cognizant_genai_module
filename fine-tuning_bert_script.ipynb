{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a50159-d353-4843-be60-1e0801d155a4",
   "metadata": {},
   "source": [
    "# Part 1: Fine-Tuning BERT\n",
    "Task: Fine-tune a pre-trained BERT model for a specific NLP task using Hugging Face.\n",
    "\n",
    "Choose an NLP task:\n",
    "\n",
    "Examples: Sentiment analysis, text classification, question answering, or named entity recognition.\n",
    "Prepare your dataset:\n",
    "\n",
    "Use a public dataset (e.g., IMDb for sentiment analysis, SQuAD for question answering).\n",
    "Ensure the dataset is preprocessed appropriately (e.g., tokenization using Hugging Face's tokenizer).\n",
    "Fine-tune BERT:\n",
    "\n",
    "Load a pre-trained BERT model from Hugging Face (e.g., bert-base-uncased).\n",
    "Set up a training loop with Hugging Face's Trainer API.\n",
    "Specify hyperparameters such as batch size, learning rate, and number of epochs.\n",
    "Monitor training:\n",
    "\n",
    "Track loss and accuracy during training.\n",
    "Save the fine-tuned model.\n",
    "Deliverable: Submit the code for fine-tuning, training logs, and a short analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce8898b1-0efc-420d-abb7-2d75c60f079c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1e13cf6-49c8-4764-9ae5-c9086361be2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TOKENIZE TRAINING DATA\n",
    "# Load the dataset\n",
    "dataset = load_dataset('squad')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(example):\n",
    "    tokenized = tokenizer(\n",
    "        example['question'],\n",
    "        example['context'],\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    # Get the start and end character positions of the answer\n",
    "    start_char = example['answers']['answer_start'][0]\n",
    "    end_char = start_char + len(example['answers']['text'][0])\n",
    "\n",
    "    # Determine the token positions\n",
    "    offsets = tokenized[\"offset_mapping\"]\n",
    "    sequence_ids = tokenized.sequence_ids()\n",
    "\n",
    "    # Find context start index\n",
    "    context_start = sequence_ids.index(1)\n",
    "    context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
    "\n",
    "    start_token = end_token = 0\n",
    "    for i in range(context_start, context_end + 1):\n",
    "        if offsets[i][0] <= start_char < offsets[i][1]:\n",
    "            start_token = i\n",
    "        if offsets[i][0] < end_char <= offsets[i][1]:\n",
    "            end_token = i\n",
    "            break\n",
    "\n",
    "    tokenized['start_positions'] = start_token\n",
    "    tokenized['end_positions'] = end_token\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6067927e-e580-4ac5-856d-a9ea2315d8a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA FOR PYTORCH\n",
    "tokenized_datasets.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=123).select(range(2000)) # Use a subset for quick training\n",
    "test_dataset = tokenized_datasets[\"validation\"].shuffle(seed=123).select(range(500))\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "m1 = BertForQuestionAnswering.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c82edb97-c964-42f4-880b-03e1dd7c3045",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 02:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.622000</td>\n",
       "      <td>3.618852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.936400</td>\n",
       "      <td>2.823883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.194100</td>\n",
       "      <td>2.618894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=3.2709960072835287, metrics={'train_runtime': 162.2887, 'train_samples_per_second': 36.971, 'train_steps_per_second': 2.311, 'total_flos': 1175835405312000.0, 'train_loss': 3.2709960072835287, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./m1results\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    ")\n",
    "\n",
    "# Define a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=m1,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84e2c32a-aa9e-4391-9b31-0dac56307cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.save_pretrained(\"models/m1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25ee86-0ad2-40d0-860f-fc51d4eb199d",
   "metadata": {},
   "source": [
    "# Part 2: Debugging Issues\n",
    "Task: Identify and resolve issues during BERT fine-tuning or prediction.\n",
    "\n",
    "Introduce or encounter common issues:\n",
    "\n",
    "Examples:\n",
    "Poor performance on validation data.\n",
    "Overfitting or underfitting.\n",
    "Long training times or memory errors.\n",
    "Analyze the problem:\n",
    "\n",
    "Review training logs and validation metrics.\n",
    "Inspect the tokenization or dataset preprocessing.\n",
    "Debug the issues:\n",
    "\n",
    "Adjust hyperparameters (e.g., learning rate, number of epochs).\n",
    "Use data augmentation or regularization techniques to address overfitting.\n",
    "Optimize memory usage by reducing batch size or gradient accumulation.\n",
    "Test the refined model:\n",
    "\n",
    "Re-run training or predictions after debugging.\n",
    "Compare results before and after debugging.\n",
    "Deliverable: Submit the initial issue, debugging steps, and improved results, with a brief explanation of your process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfb478cc-b8fd-4a7e-a474-19b045e02c54",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA FOR PYTORCH\n",
    "tokenized_datasets.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=123).select(range(2000)) # Use a subset for quick training\n",
    "test_dataset = tokenized_datasets[\"validation\"].shuffle(seed=123).select(range(500))\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "m2 = BertForQuestionAnswering.from_pretrained('bert-base-uncased').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f17c8c8-f697-417f-ac2e-ee91eb622929",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 03:19, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.380500</td>\n",
       "      <td>4.104551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.742400</td>\n",
       "      <td>3.676965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.270100</td>\n",
       "      <td>3.401626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.832500</td>\n",
       "      <td>2.936658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.070500</td>\n",
       "      <td>2.598091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.944200</td>\n",
       "      <td>2.432039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.660900</td>\n",
       "      <td>2.353867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.463800</td>\n",
       "      <td>2.339232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.413200</td>\n",
       "      <td>2.328587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.154200</td>\n",
       "      <td>2.337282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=2.4482653081417083, metrics={'train_runtime': 200.7604, 'train_samples_per_second': 99.621, 'train_steps_per_second': 1.594, 'total_flos': 3919451351040000.0, 'train_loss': 2.4482653081417083, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./m2results\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.05,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    fp16=True,\n",
    "    no_cuda=False\n",
    ")\n",
    "\n",
    "# Define a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=m2,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2654ec3-62ff-47e0-9502-634ba179cc69",
   "metadata": {},
   "source": [
    "Tokenizing procedure remained unchanged.\n",
    "Increasing epochs shows positive growth that stagnates around 10 epochs in, kept learning rate as it was but slower learning rates of 1 or 1.5 benefit more around 12 epochs in. Batch size is halved for memory optimization and gradient accumulation is introduced. Decay rate increased to 0.05 to combat overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae71cf29-a42d-4085-b0d2-605b25dd925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.save_pretrained(\"models/m2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2b539-854d-4f2e-b2a8-1e4c8044916c",
   "metadata": {},
   "source": [
    "# Part 3: Evaluating the Model\n",
    "Task: Use evaluation metrics to assess the fine-tuned BERT model.\n",
    "\n",
    "Generate predictions on a test set:\n",
    "\n",
    "Use the fine-tuned model to make predictions on unseen data.\n",
    "Evaluate performance using these metrics:\n",
    "\n",
    "Accuracy: For classification tasks.\n",
    "F1-Score: Balance of precision and recall.\n",
    "Exact Match (EM): For question answering tasks.\n",
    "Mean Squared Error (MSE): For regression tasks.\n",
    "Log Loss: For probabilistic outputs.\n",
    "Refine the model:\n",
    "\n",
    "Based on evaluation results, adjust the model (e.g., by refining prompts, hyperparameters, or preprocessing).\n",
    "Deliverable: Submit evaluation metrics, a comparison of results before and after refinement, and a reflection on the improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "647badb3-f87c-4c3e-a1a1-1c65b69d7279",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TOKENIZE TEST DATA\n",
    "# Load the dataset, sampled size not present in training validation set\n",
    "dataset = load_dataset('squad')['validation'].shuffle(seed=123).select(range(500, 1012))\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_test_set(examples):\n",
    "    # Tokenize using the question and context\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Needed to align overflowed tokens back to examples\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "\n",
    "    # Prepare extra fields\n",
    "    tokenized_examples[\"id\"] = []\n",
    "    tokenized_examples[\"context\"] = []\n",
    "    tokenized_examples[\"answers\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"id\"].append(examples[\"id\"][sample_index])\n",
    "        tokenized_examples[\"context\"].append(examples[\"context\"][sample_index])\n",
    "        tokenized_examples[\"answers\"].append(examples[\"answers\"][sample_index])\n",
    "\n",
    "        # Set to None for question and special tokens\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else (-1, 1) for k, o in enumerate(offset_mapping[i])\n",
    "        ]\n",
    "\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "# Apply to entire validation set\n",
    "from datasets import Dataset\n",
    "tokenized_test_set = dataset.map(\n",
    "    tokenize_test_set,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "tokenized_test_set.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
    "    output_all_columns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "945c8755-3505-4fe4-b22e-c53ec37f520e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR PREDICTION AND INTERPRETATION\n",
    "def get_predictions(model, tokenized_test_set, batch_size=16):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_test_set.remove_columns([\"offset_mapping\", \"context\", \"answers\", \"id\"]),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    print(\"Dataloader created successfully.\")\n",
    "\n",
    "    start_logits_all = []\n",
    "    end_logits_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            start_logits_all.append(outputs.start_logits.cpu())\n",
    "            end_logits_all.append(outputs.end_logits.cpu())\n",
    "    \n",
    "    start_logits = torch.cat(start_logits_all, dim=0)\n",
    "    end_logits = torch.cat(end_logits_all, dim=0)\n",
    "\n",
    "    return start_logits, end_logits\n",
    "    \n",
    "def decode_predictions(start_logits, end_logits, tokenized_dataset, tokenizer, n_best_size=1, max_answer_length=30):\n",
    "    print(\"Starting decode_predictions...\")\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(len(start_logits)):\n",
    "\n",
    "        if i % 128 == 0:\n",
    "            print(f\"Decoding sample {i}/{len(start_logits)}\")\n",
    "            \n",
    "        input_ids = tokenized_dataset[i]['input_ids']\n",
    "        offset_mapping = tokenized_dataset[i]['offset_mapping']\n",
    "        context = tokenized_dataset[i]['context']\n",
    "        qas_id = tokenized_dataset[i]['id']\n",
    "        \n",
    "        start_logit = start_logits[i]\n",
    "        end_logit = end_logits[i]\n",
    "\n",
    "        # Get the best start-end span\n",
    "        max_score = float('-inf')\n",
    "        best_span = (0, 0)\n",
    "        for start_index in range(len(start_logit)):\n",
    "            for end_index in range(start_index, min(len(end_logit), start_index + max_answer_length)):\n",
    "                score = start_logit[start_index] + end_logit[end_index]\n",
    "                if score > max_score and offset_mapping[start_index] is not None and offset_mapping[end_index] is not None:\n",
    "                    max_score = score\n",
    "                    best_span = (start_index, end_index)\n",
    "\n",
    "        # Decode answer\n",
    "        start_char = offset_mapping[best_span[0]][0]\n",
    "        end_char = offset_mapping[best_span[1]][1]\n",
    "        answer = context[start_char:end_char]\n",
    "        predictions.append({\n",
    "            \"id\": qas_id,\n",
    "            \"prediction_text\": answer\n",
    "        })\n",
    "\n",
    "    print(\"Finished decode_predictions.\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "672f3949-460e-4e1d-b57b-9294e8f78e87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████████████████████████████████████████████████████████████████| 33/33 [00:03<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting decode_predictions...\n",
      "Decoding sample 0/524\n",
      "Decoding sample 128/524\n",
      "Decoding sample 256/524\n",
      "Decoding sample 384/524\n",
      "Decoding sample 512/524\n",
      "Finished decode_predictions.\n",
      "m1 results\n",
      "EM: 32.23\n",
      "F1: 45.18\n",
      "\n",
      "Dataloader created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████████████████████████████████████████████████████████████████| 33/33 [00:03<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting decode_predictions...\n",
      "Decoding sample 0/524\n",
      "Decoding sample 128/524\n",
      "Decoding sample 256/524\n",
      "Decoding sample 384/524\n",
      "Decoding sample 512/524\n",
      "Finished decode_predictions.\n",
      "m2 results\n",
      "EM: 42.19\n",
      "F1: 54.08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION\n",
    "for file in os.listdir('models/'):\n",
    "    model = BertForQuestionAnswering.from_pretrained('models/' + str(file))\n",
    "    start_logits, end_logits = get_predictions(model, tokenized_test_set)\n",
    "    predictions = decode_predictions(start_logits, end_logits, tokenized_test_set, tokenizer)\n",
    "    \n",
    "    # Convert predictions and references to dictionaries keyed by ID\n",
    "    pred_dict = {p['id']: p for p in predictions}\n",
    "    ref_dict = {r['id']: r for r in references}\n",
    "    \n",
    "    # Keep only IDs present in BOTH\n",
    "    common_ids = set(pred_dict.keys()) & set(ref_dict.keys())\n",
    "    \n",
    "    # Filter to aligned lists\n",
    "    aligned_preds = [pred_dict[i] for i in common_ids]\n",
    "    aligned_refs = [ref_dict[i] for i in common_ids]\n",
    "    \n",
    "    metric = evaluate.load(\"squad\")\n",
    "    references = [\n",
    "        {\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"answers\": ex[\"answers\"]\n",
    "        } for ex in dataset\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=aligned_preds, references=aligned_refs)\n",
    "    print(str(file), 'results')\n",
    "    print(f\"EM: {results['exact_match']:.2f}\")\n",
    "    print(f\"F1: {results['f1']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c84e59-f126-4cea-9881-578b9bfdc5b7",
   "metadata": {},
   "source": [
    "# Part 4: Creative Application\n",
    "Task: Apply BERT to solve a real-world NLP problem.\n",
    "\n",
    "Choose a creative NLP task:\n",
    "\n",
    "Examples:\n",
    "Classify customer reviews as positive or negative.\n",
    "Extract key entities (e.g., names, dates) from legal documents.\n",
    "Answer questions based on a given passage of text.\n",
    "Build and fine-tune your BERT model:\n",
    "\n",
    "Use Hugging Face's model hub to experiment with different BERT variants (e.g., distilbert-base-uncased, bert-large-cased).\n",
    "Use advanced techniques like data augmentation, early stopping, or mixed precision training.\n",
    "Debug and evaluate the model:\n",
    "\n",
    "Troubleshoot issues and ensure the model performs well on the chosen task.\n",
    "Deliverable: Submit the final fine-tuned BERT model, evaluation metrics, and a summary of the techniques you used to achieve the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8f68908-7cf8-4a1e-8ab1-09ac3c32c6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: october\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('models/m2')\n",
    "\n",
    "question = \"What month is my birthday?\"\n",
    "context = \"My birthday is in October. It is not in August. My sister's birthday is in May.\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits) + 1\n",
    "\n",
    "answer_tokens = inputs['input_ids'][0][start_index:end_index]\n",
    "answer = tokenizer.decode(answer_tokens)\n",
    "print(\"Answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognizant",
   "language": "python",
   "name": "cognizant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
